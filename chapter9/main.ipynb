{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283a7892-99b9-4efb-a05b-297834fc85bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U langsmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7c787947-918e-4a18-9d9e-6f296358762a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 71 in the collection\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "loader = TextLoader('source.txt')\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,)\n",
    "\n",
    "split_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "chroma_document_store = Chroma.from_documents(split_documents, embeddings)\n",
    "\n",
    "print(\"There are\", chroma_document_store._collection.count(), \"in the collection\")\n",
    "\n",
    "retriever = chroma_document_store.as_retriever(search_kwargs={\"k\": 4})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "578a0e4f-3f7f-46ed-bd3b-dec5590596c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The documents provided do not contain specific information about the effects of eating beetroot on running performance or endurance.'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n",
    "\n",
    "# Define prompt template\n",
    "template = \"\"\"You are a helpful documentation Q&A assistant, trained to answer questions about ultra marathon running. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use two sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Setup RAG pipeline\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke({\"question\": \"will eating beetroot make me run further\"})\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "9f7ebb55-4743-4597-a167-19015d8cb0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langsmith import Client\n",
    "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
    "from langsmith.schemas import Run, Example\n",
    "\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"\"\n",
    "os.environ['OPENAI_API_KEY'] = ''\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = \"true\"\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = \"https://api.smith.langchain.com\"\n",
    "os.environ['LANGCHAIN_PROJECT'] = \"book-chapter-9\" \n",
    "\n",
    "\n",
    "# Initialize the LangSmith client\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1a5a7a12-56a6-41b8-8a5a-43d3da343f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset\n",
    "dataset_name = \"Ultra Marathon QA Dataset\"\n",
    "description = \"Dataset for evaluating QA correctness of an ultra marathon chatbot.\"\n",
    "\n",
    "# Create the dataset in LangSmith\n",
    "dataset = client.create_dataset(dataset_name, description=description)\n",
    "\n",
    "# Add examples to the dataset\n",
    "client.create_examples(\n",
    "    inputs=[\n",
    "        {\"question\": \"How do you train for an ultra marathon?\"},\n",
    "        {\"question\": \"will i need to buy different sized trainers?\"}\n",
    "    ],\n",
    "    outputs=[\n",
    "        {\"answer\": \"Training involves increasing mileage gradually with long runs and recovery periods.\"},\n",
    "        {\"answer\": \"Consider buying trainers that are a half-size larger to accommodate swelling feet during long runs.\"}\n",
    "    ],\n",
    "    dataset_id=dataset.id,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "eb2af8a0-5fee-4a3c-aa5b-f2a9e7f65f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'Chat Single Turn-8f72e190' at:\n",
      "https://smith.langchain.com/o/d0accd60-ea2b-5fb1-8194-a9f975563f7e/datasets/0eb55f05-964c-444e-b69b-d85b5ed02faa/compare?selectedSessions=fe12e2ae-43b8-409a-9891-8a8dce6e289d\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:18,  6.28s/it]\n"
     ]
    }
   ],
   "source": [
    "def predict(inputs: dict):\n",
    "    return rag_chain.invoke({\"question\": inputs[\"question\"]})\n",
    "\n",
    "\n",
    "def format_evaluator_inputs(run: Run, example: Example):\n",
    "    return {\n",
    "        \"input\": example.inputs[\"question\"],\n",
    "        \"prediction\": next(iter(run.outputs.values())),\n",
    "        \"reference\": example.outputs[\"answer\"],\n",
    "    }\n",
    "\n",
    "\n",
    "correctness_evaluator = LangChainStringEvaluator(\n",
    "    \"labeled_score_string\",\n",
    "    config={\"criteria\": \"correctness\", \"normalize_by\": 10},\n",
    "    prepare_data=format_evaluator_inputs,\n",
    ")\n",
    "\n",
    "results = evaluate(\n",
    "    predict,\n",
    "    data=dataset_name,\n",
    "    experiment_prefix=\"Chat Single Turn\",\n",
    "    evaluators=[correctness_evaluator],\n",
    "    metadata={\"model\": \"gpt-3.5-turbo\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "cc54c8ba-a31c-49c9-a84c-ad9322263c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'run007' at:\n",
      "https://smith.langchain.com/o/d0accd60-ea2b-5fb1-8194-a9f975563f7e/datasets/d1a15a3e-0566-4268-85f0-a261ff4f426b/compare?selectedSessions=4bdb80f7-40b4-4fe5-a7d0-511222237784\n",
      "\n",
      "View all tests for Dataset Ultra Marathon QA Dataset at:\n",
      "https://smith.langchain.com/o/d0accd60-ea2b-5fb1-8194-a9f975563f7e/datasets/d1a15a3e-0566-4268-85f0-a261ff4f426b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example 5d2532f7-3b56-4392-9028-f6cfcc5c58ee with inputs {'input': 'will i need to buy different sized running shoes'}\n",
      "Error Type: KeyError, Message: 'question'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------------------------------------------->] 1/1"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>Experiment Results:</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>error</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>run_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>'question'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b2a683a2-0b62-4950-903c-e2003d1851d3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.063151</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.063151</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.063151</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.063151</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.063151</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.063151</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             error  execution_time                                run_id\n",
       "count            1        1.000000                                     1\n",
       "unique           1             NaN                                     1\n",
       "top     'question'             NaN  b2a683a2-0b62-4950-903c-e2003d1851d3\n",
       "freq             1             NaN                                     1\n",
       "mean           NaN        0.063151                                   NaN\n",
       "std            NaN             NaN                                   NaN\n",
       "min            NaN        0.063151                                   NaN\n",
       "25%            NaN        0.063151                                   NaN\n",
       "50%            NaN        0.063151                                   NaN\n",
       "75%            NaN        0.063151                                   NaN\n",
       "max            NaN        0.063151                                   NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = evaluate(\n",
    "    chatbot_task,            # Your chatbot task function\n",
    "    dataset_name,            # The dataset to evaluate on\n",
    "    [correctness_evaluator]  # The evaluator function\n",
    ")\n",
    "\n",
    "# import langsmith\n",
    "# from langchain import chat_models, prompts, smith\n",
    "# from langchain.schema import output_parser\n",
    "\n",
    "\n",
    "\n",
    "# # Define the evaluators to apply\n",
    "# eval_config = smith.RunEvalConfig(\n",
    "#     evaluators=[\n",
    "#         \"qa\"\n",
    "#     ],\n",
    "#     eval_llm=chat_models.ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "# )\n",
    "\n",
    "\n",
    "# chain_results = client.run_on_dataset(\n",
    "#     dataset_name=\"Ultra Marathon QA Dataset\",\n",
    "#     llm_or_chain_factory=rag_chain,\n",
    "#     evaluation=eval_config,\n",
    "#     project_name=\"run007\",\n",
    "#     concurrency_level=5,\n",
    "#     verbose=True,\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
