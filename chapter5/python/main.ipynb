{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5bf1a3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Certainly! Here are some available options for direct flights from New York to London departing on December 10th and returning on January 5th:\\n\\n1. British Airways:\\n   - Departure: December 10th, 6:00 PM from JFK Airport\\n   - Return: January 5th, 8:30 AM from Heathrow Airport\\n   - Price: $850\\n\\n2. Virgin Atlantic:\\n   - Departure: December 10th, 7:30 PM from Newark Airport\\n   - Return: January 5th, 9:00 AM from Heathrow Airport\\n   - Price: $900\\n\\n3. American Airlines:\\n   - Departure: December 10th, 8:00 PM from JFK Airport\\n   - Return: January 5th, 10:30 AM from Heathrow Airport\\n   - Price: $950\\n\\n4. Delta Air Lines:\\n   - Departure: December 10th, 9:30 PM from JFK Airport\\n   - Return: January 5th, 11:00 AM from Heathrow Airport\\n   - Price: $920\\n\\nPlease note that prices are subject to change and availability may vary. It's always recommended to check with the airlines or a trusted travel agency for the most up-to-date information and to book your preferred flight.\")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import langchain\n",
    "import openai\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage \n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = ''\n",
    "\n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = \"true\"\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = \"https://api.smith.langchain.com\"\n",
    "os.environ['LANGCHAIN_API_KEY'] = ''\n",
    "\n",
    "chat_model = ChatOpenAI(temperature=0.0,openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "chat_model\n",
    "chat_model.predict(\"hi!\")\n",
    "\n",
    "text = \"I am looking to book a direct flight from New York to London departing on December 10th and returning on January 5th. Can you provide me with the available options, including airlines, flight times, and prices\" \n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You're a helpful flight expert\"),\n",
    "    HumanMessage(content=text),\n",
    "]\n",
    "\n",
    "chat_model.predict_messages(messages) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c13260f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Look at the following conversation {conversation} from the following service area {service_area} on {event_date_time} and returna sentiment\"\n",
    ")\n",
    "\n",
    "conversation = \"\"\"Customer: My new bike is missing a wheel!\\\n",
    "Chatbot: I'm sorry to hear that. Could I have your order number, please?\\\n",
    "Customer: It's #54321.\\\n",
    "Chatbot: Thank you. We'll send a replacement wheel today, and it'll arrive in two days.\\\n",
    "Customer: Make sure it does. This has been a hassle.\\\n",
    "Chatbot: Understandably so, and we apologize. You’ll also get a 20% discount on your next order for the inconvenience.\\\n",
    "Customer: Fine, thank you.\\\n",
    "Chatbot: You're welcome, and the confirmation is on its way. If there’s more I can do for you, just let me know.\"\"\"\n",
    "\n",
    "reminder_message = prompt_template.format(\n",
    "    conversation=conversation,\n",
    "    service_area=\"complaints\",\n",
    "    event_date_time=\"2023-10-19 14:30:00\"\n",
    ")\n",
    "reminder_message\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853cf89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate \n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages( \n",
    "\n",
    "    [ \n",
    "\n",
    "(\"system\", \"You are a health advisory bot for HealthHub Clinic. You can answer questions from the patient called {name}\"), \n",
    "\n",
    "(\"ai\", \"Hi, {name} please ask me your question.\"), \n",
    "\n",
    " (\"human\", \"{user_input}\"), \n",
    "\n",
    "    ] \n",
    "\n",
    ") \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "messages = chat_template.format_messages(name=\"Lucy\", user_input=\"What are the symptoms of the flu?\")\n",
    "\n",
    "messages\n",
    "\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "llm(messages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9947ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, validator\n",
    "\n",
    "class ProductivityChatPromptTemplate(ChatPromptTemplate, BaseModel):\n",
    "    @validator(\"input_variables\")\n",
    "    def validate_input_variables(cls, v):\n",
    "        required_vars = {\"task\", \"time_available\", \"user_preferences\"}\n",
    "        if not required_vars.issubset(v):\n",
    "            raise ValueError(f\"Input variables must include: {required_vars}\")\n",
    "        return v\n",
    "\n",
    "    def format_messages(self, **kwargs) -> str:\n",
    "        messages = [\n",
    "            (\"system\", \"You are a virtual productivity assistant.\"),\n",
    "            (\"human\", f\"I need to {kwargs['task']} and I only have {kwargs['time_available']}.\"),\n",
    "            (\"human\", f\"My preference is to {kwargs['user_preferences']}.\"),\n",
    "            (\"ai\", \"Based on your task and preferences, here's my advice:\")\n",
    "            # The AI's response would be generated by the language model following this prompt.\n",
    "        ]\n",
    "        return self.construct_chat(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482058ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "\n",
    "def get_source_code(function_name):\n",
    "    # Get the source code of the function\n",
    "    return inspect.getsource(function_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d53704",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import StringPromptTemplate\n",
    "from pydantic import BaseModel, validator\n",
    "\n",
    "PROMPT = \"\"\"\\\n",
    "Given the function name and source code, generate an English language explanation of the function.\n",
    "Function Name: {function_name}\n",
    "Source Code:\n",
    "{source_code}\n",
    "Explanation:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class FunctionExplainerPromptTemplate(StringPromptTemplate, BaseModel):\n",
    "    \"\"\"A custom prompt template that takes in the function name as input, and formats the prompt template to provide the source code of the function.\"\"\"\n",
    "\n",
    "    @validator(\"input_variables\")\n",
    "    def validate_input_variables(cls, v):\n",
    "        \"\"\"Validate that the input variables are correct.\"\"\"\n",
    "        if len(v) != 1 or \"function_name\" not in v:\n",
    "            raise ValueError(\"function_name must be the only input_variable.\")\n",
    "        return v\n",
    "\n",
    "    def format(self, **kwargs) -> str:\n",
    "        # Get the source code of the function\n",
    "        source_code = get_source_code(kwargs[\"function_name\"])\n",
    "\n",
    "        # Generate the prompt to be sent to the language model\n",
    "        prompt = PROMPT.format(\n",
    "            function_name=kwargs[\"function_name\"].__name__, source_code=source_code\n",
    "        )\n",
    "        return prompt\n",
    "\n",
    "    def _prompt_type(self):\n",
    "        return \"function-explainer\"\n",
    "    \n",
    "    \n",
    "fn_explainer = FunctionExplainerPromptTemplate(input_variables=[\"function_name\"])\n",
    "\n",
    "# Generate a prompt for the function \"get_source_code\"\n",
    "prompt = fn_explainer.format(function_name=get_source_code)\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d2efec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filename: Parsing to a JSON dictionary  \n",
    "\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "intent_schema = ResponseSchema(name=\"intents\",\n",
    "                             description=\"Format the output as JSON object consisting of a key: intents, the intents key will be a list of objects with the following keys: intent, utterance, category\")\n",
    "response_schemas = [intent_schema]\n",
    "\n",
    "intent_output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "intent_format_instructions = intent_output_parser.get_format_instructions()\n",
    "\n",
    "print(intent_format_instructions)\n",
    "\n",
    "intent_template = \"\"\"\n",
    "Create intents and utterances for a chatbot which will answer questions about a college, \\ \n",
    "Create 10 examples of intents for the following categories: facilities and course_information \\\n",
    " ensure that each intent has 10 utterances, create 5 long tail and 5 more common utterances \\\n",
    "\n",
    "{intent_format_instructions}\n",
    "\n",
    "\"\"\"\n",
    "prompt_template = ChatPromptTemplate.from_template(intent_template)\n",
    "                                                           \n",
    "messages = prompt_template.format_messages(intent_format_instructions=intent_format_instructions)\n",
    "#messages = prompt_template.format_messages(intent_examples=intents)\n",
    "chat = ChatOpenAI(temperature=0.0, model=\"gpt-4\")\n",
    "response = chat(messages)\n",
    "\n",
    "# this will be type str\n",
    "type(response.content)\n",
    "                               \n",
    "output_dict = intent_output_parser.parse(response.content)\n",
    "\n",
    "output_dict.get('intents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fb6fff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='\\n    Translate this text: what is the capital of england to French \\n')])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filename: Basic Langchain chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    Translate this text: {text} to {language} \n",
    "\"\"\"\n",
    ")\n",
    "language = \"French\"\n",
    "text = \"what is the capital of england\"\n",
    "runnable  = prompt\n",
    "runnable.invoke({\"text\":text,\"language\":language})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab1bc6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filename: Sequential chain transcript processor \n",
    "import json\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "# Initialize ChatGPT model\n",
    "chat_model = ChatOpenAI(temperature=0, model=\"gpt-4-1106-preview\")\n",
    "\n",
    "# Load transcripts data\n",
    "with open('transcripts.json', 'r') as file:\n",
    "    transcripts = json.load(file)\n",
    "\n",
    "# Define response schemas\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"transactional_transcripts\", description=\"Format the output as JSON list of conversations with the same JSON format as the input,add an category key to each conversation\", type=\"list\"),\n",
    "    ResponseSchema(name=\"faq\", description=\"Format the output as JSON list of conversations with the same JSON format as the input, add an category key to each conversation \", type=\"list\"),\n",
    "]\n",
    "\n",
    "# Define chat transcript template with placeholders for transcripts and format instructions\n",
    "transcript_template = \"Look at the following chat transcripts {transcripts} and categorize them into FAQ and transactional conversations in the following format {format_instructions}\"\n",
    "\n",
    "\n",
    "# Define prompt templates\n",
    "transactional_categorization_prompt_template = HumanMessagePromptTemplate.from_template(transcript_template)\n",
    "\n",
    "# Create output parser\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "# Get format instructions\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "# Create prompts\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[transactional_categorization_prompt_template],\n",
    "    input_variables=[\"transcripts\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "# Define chain for transactional categorization\n",
    "chain_one = prompt | chat_model | output_parser\n",
    "\n",
    "#chain_one_result = chain_one.invoke({\"transcripts\": transcripts})\n",
    "\n",
    "# Define intent response schemas\n",
    "intent_response_schemas = [\n",
    "    ResponseSchema(name=\"transactional_intents\", description=\"Format the output as JSON list of conversation transcripts using the same format from the transactional_transcript list, add an intent key to each conversation\", type=\"list\"),\n",
    "]\n",
    "\n",
    "intent_transcript_template = \"Look at the following chat transcripts {transactional_transcripts} Cluster these conversations by intent {intent_format_instructions}\"\n",
    "\n",
    "\n",
    "# Create intent prompt\n",
    "intent_clustering_prompt_template = HumanMessagePromptTemplate.from_template(intent_transcript_template)\n",
    "\n",
    "# Create intent output parser\n",
    "intent_output_parser = StructuredOutputParser.from_response_schemas(intent_response_schemas)\n",
    "\n",
    "# Get intent format instructions\n",
    "intent_format_instructions = intent_output_parser.get_format_instructions()\n",
    "\n",
    "# Create prompt for intent clustering\n",
    "prompt_two = ChatPromptTemplate(\n",
    "    messages=[intent_clustering_prompt_template],\n",
    "    input_variables=[\"transactional_transcripts\"],\n",
    "    partial_variables={\"intent_format_instructions\": intent_format_instructions},\n",
    ")\n",
    "\n",
    "# Create chain for intent clustering\n",
    "chain2 = (\n",
    "    {\"transactional_transcripts\": chain_one}\n",
    "    | prompt_two\n",
    "    | chat_model\n",
    "    | intent_output_parser\n",
    ")\n",
    "\n",
    "# Pass transcripts through chain inputs\n",
    "chain_two_result = chain2.invoke({\"transcripts\": transcripts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a71587f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'live_chat_transcript': [{'role': 'customer', 'message': 'Hello, I need to schedule a maintenance appointment for my ElectricZ Model 3. Can you help with that?'}, {'role': 'support_agent', 'message': \"Certainly! Can you please provide your car's VIN and let me know what type of maintenance you require?\"}, {'role': 'customer', 'message': 'My VIN is XYZ12345, and I need a routine checkup.'}, {'role': 'support_agent', 'message': \"Thank you! I've scheduled your maintenance appointment. You'll receive a confirmation email shortly. Is there anything else I can assist you with?\"}], 'category': 'transactional', 'intent': 'Schedule Maintenance'}\n"
     ]
    }
   ],
   "source": [
    "print(chain_two_result.get('transactional_intents')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "993912c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Recent trends in artificial intelligence (AI) highlight its expanding role in various sectors and the associated ethical considerations. In healthcare, AI is improving patient outcomes through predictive analytics, diagnostic assistance, and treatment personalization, with capabilities matching those of human experts in some cases. Ethical AI developments are gaining attention, with a focus on creating transparent, unbiased systems that respect privacy, alongside calls for stricter regulations. AI's involvement in creative industries raises questions about its impact on creativity, while its role in automation sparks debates on job displacement and creation.\\n\\nAdvancements in natural language processing (NLP) are notable, with AI models like GPT-3 generating human-like text for applications such as customer service and coding. Autonomous vehicles continue to progress, facing technological and regulatory challenges. Robotic process automation (RPA) is streamlining business operations, and AI is contributing to climate change efforts by modeling scenarios and enhancing renewable energy systems. AI is also becoming a critical tool in cybersecurity and is being processed locally on devices for speed and privacy improvements.\\n\\nAcademic research on AI is diverse, focusing on machine learning algorithm enhancements, AI fairness and ethics, and explainable AI (XAI) to improve system transparency. AI in healthcare is a significant research area, as is NLP, which is advancing language understanding. AI's role in addressing climate change, the exploration of neuro-AI and cognitive computing, advancements in AI hardware for computational efficiency, and robotics and autonomous systems are also key research topics. AI in cybersecurity is being explored to automate threat detection and response.\\n\\nGeneral information from the web defines AI as a field that creates intelligent machines through techniques like machine learning, NLP, robotics, and computer vision. AI applications span healthcare, finance, transportation, retail, manufacturing, and entertainment. Ethical challenges include algorithmic bias and job displacement concerns, with a need for responsible AI development. The future of AI points towards more advanced machine learning, integration with IoT, and a broader societal impact.\\n\\nOverall, AI is becoming increasingly integrated into various industries, prompting ongoing research, ethical considerations, and discussions about its future role in society.\"\n"
     ]
    }
   ],
   "source": [
    "#filename: Parallel Chains in Langchain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnableParallel\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4-1106-preview\")\n",
    "\n",
    "# prompt for querying news articles about a topic\n",
    "news_prompt = ChatPromptTemplate.from_template(\"summarize recent news articles about {topic}\")\n",
    "\n",
    "# Chain for querying scientific papers about a topic\n",
    "academic_prompt = ChatPromptTemplate.from_template(\"summarize recent scientific papers about {topic}\")\n",
    "\n",
    "# Chain for querying general web information about a topic\n",
    "web_info_prompt = ChatPromptTemplate.from_template(\"provide a general overview of {topic} from web sources\")\n",
    "\n",
    "\n",
    "# Create a RunnableParallel instance with the three chains\n",
    "# parallel_chain = RunnableParallel(news=news_chain, academic=academic_chain, web_info=web_info_chain)\n",
    "\n",
    "parallel = RunnableParallel(\n",
    "    news = news_prompt | model,\n",
    "    academic = academic_prompt | model,\n",
    "    web_info = web_info_prompt | model\n",
    ")\n",
    "\n",
    "summarise_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "summarize the following information from these different sources:\n",
    "\n",
    "News source: {news}\n",
    "Academic: {academic}\n",
    "Web: {web_info}\n",
    "\n",
    "Summary:\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "# Invoke the parallel chain with a specific topic\n",
    "#results = parallel_chain.invoke({\"topic\": \"artificial intelligence\"})\n",
    "\n",
    "summarise_chain = parallel | summarise_prompt | model\n",
    "\n",
    "summarise_output = summarise_chain.invoke({\"topic\": \"artificial intelligence\"})\n",
    "\n",
    "print(summarise_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6dfd975c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The joke is about bears never getting caught gambling because they always play with a \"bear\" minimum. The poem describes bears as wild and mighty guardians of nature that roam freely in deep forests.'\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate  \n",
    "from langchain.schema.runnable import RunnableParallel, RunnableSequence\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "joke_prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "poem_prompt = ChatPromptTemplate.from_template(\"write a 2-line poem about {topic}\")\n",
    "\n",
    "parallel = RunnableParallel(\n",
    "    joke = joke_prompt | model,\n",
    "    poem = poem_prompt | model\n",
    ")\n",
    "\n",
    "summary_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Summarize the joke and poem about\n",
    "\n",
    "Joke: {joke}\n",
    "Poem: {poem}\n",
    "\n",
    "Summary:\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "sequence = parallel | summary_prompt | model\n",
    "\n",
    "output = sequence.invoke({\"topic\": \"bears\"})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df300abc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A subscription car, also known as a car subscription service or car-as-a-service, is a relatively new concept in the automotive industry. It offers an alternative to traditional car ownership or leasing by providing individuals with a flexible and convenient way to access a vehicle.\\n\\nThe benefits of a subscription car can vary depending on the specific service provider, but here are some common advantages:\\n\\n1. Flexibility: Car subscriptions typically offer more flexibility compared to traditional ownership or leasing options. Subscribers can often choose the duration of their subscription, ranging from a few weeks to several months, and may have the option to switch to different vehicles within the service's fleet.\\n\\n2. Convenience: Car subscriptions often include various services in a single monthly payment, such as vehicle registration, insurance, maintenance, and roadside assistance. This eliminates the need for separate arrangements and simplifies the overall ownership experience.\\n\\n3. Cost Savings: While the monthly cost of a car subscription may initially seem higher than a traditional lease payment, it can be more cost-effective in certain situations. Car subscriptions typically include all major expenses, such as insurance and maintenance, which can save subscribers money in the long run.\\n\\n4. Access to Different Vehicles: Subscribers often have the opportunity to drive different vehicles within the service provider's fleet, allowing them to experience various models and types of cars without the commitment of ownership.\\n\\n5. No Long-Term Commitment: Unlike traditional leasing or financing, car subscriptions generally do not require a long-term commitment. This can be beneficial for individuals who have short-term needs for a vehicle or those who prefer not to be tied to a single car for an extended period.\\n\\n6. Reduced Hassle: Car subscriptions eliminate the hassle associated with selling or trading in a vehicle when you no longer need it. Once the subscription period ends, you can simply return the car to the service provider without any further obligations.\\n\\nIt's important to note that the availability and specific benefits of car subscriptions can vary depending on the service provider, location, and individual needs. If you are interested in a car subscription, it is recommended to research and compare different providers to find the one that best suits your requirements.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filename: Routing Chains\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableBranch\n",
    "from langchain.prompts import PromptTemplate\n",
    "from typing import Literal\n",
    "from langchain.output_parsers.openai_functions import PydanticAttrOutputFunctionsParser\n",
    "from langchain.pydantic_v1 import BaseModel\n",
    "from langchain.utils.openai_functions import convert_pydantic_to_openai_function\n",
    "from operator import itemgetter\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "# Initialize the language model\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4-1106-preview\")\n",
    "\n",
    "# Maintenance Department Template\n",
    "maintenance_template = \"\"\"You are an assistant at a car service center. \\\n",
    "You help customers book their cars for service. You will collect details such as the customer's name, \\\n",
    "car registration number, and their preferred date and time for the car collection.\n",
    "\n",
    "Here is the service booking request:\n",
    "{input}\"\"\"\n",
    "maintenance_prompt = PromptTemplate.from_template(maintenance_template)\n",
    "\n",
    "# Car Information Department Template\n",
    "car_info_template = \"\"\"You are knowledgeable about various car models and their features. \\\n",
    "You can provide detailed information about car specifications, models, and performance. \\\n",
    "If a question is outside your expertise, you recommend contacting the car manufacturer.\n",
    "\n",
    "Here is the question:\n",
    "{input}\"\"\"\n",
    "car_info_prompt = PromptTemplate.from_template(car_info_template)\n",
    "\n",
    "# Accounts Department Template\n",
    "accounts_template = \"\"\"You are well-versed in account management for car subscriptions. \\\n",
    "You can answer questions about billing, payment methods, and subscription plans. \\\n",
    "In cases of specific account issues, you advise contacting the accounts department directly.\n",
    "\n",
    "Here is the question:\n",
    "{input}\"\"\"\n",
    "accounts_prompt = PromptTemplate.from_template(accounts_template)\n",
    "\n",
    "# General Prompt Template for Other Queries\n",
    "general_prompt = PromptTemplate.from_template(\n",
    "    \"You are a helpful assistant. Answer the FAQ question as accurately as you can.\\n\\n{input}\"\n",
    ")\n",
    "\n",
    "# Branching Logic Based on Department\n",
    "prompt_branch = RunnableBranch(\n",
    "    (lambda x: x[\"topic\"] == \"maintenance\", maintenance_prompt),\n",
    "    (lambda x: x[\"topic\"] == \"car_info\", car_info_prompt),\n",
    "    (lambda x: x[\"topic\"] == \"accounts\", accounts_prompt),\n",
    "    general_prompt,\n",
    ")\n",
    "\n",
    "\n",
    "# Topic Classifier for Department Selection\n",
    "class TopicClassifier(BaseModel):\n",
    "    \"Classify the topic of the user question\"\n",
    "    topic: Literal[\"maintenance\", \"car_info\", \"accounts\", \"general\"]\n",
    "    \"The topic of the user question. One of 'maintenance', 'car_info', 'accounts', or 'general'.\"\n",
    "\n",
    "classifier_function = convert_pydantic_to_openai_function(TopicClassifier)\n",
    "llm = ChatOpenAI().bind(\n",
    "    functions=[classifier_function], function_call={\"name\": \"TopicClassifier\"}\n",
    ")\n",
    "parser = PydanticAttrOutputFunctionsParser(\n",
    "    pydantic_schema=TopicClassifier, attr_name=\"topic\"\n",
    ")\n",
    "classifier_chain = llm | parser\n",
    "\n",
    "    \n",
    "# Final Chain Assembly\n",
    "final_chain = (\n",
    "    RunnablePassthrough.assign(topic=itemgetter(\"input\") | classifier_chain)\n",
    "    | prompt_branch\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Example Invocation\n",
    "final_chain.invoke(\n",
    "    #{\"input\": \"How do I update my payment method for my car subscription?\"}\n",
    "    #{\"input\": \"which is car with the longest range battery?\"}\n",
    "    {\"input\": \"whats the benefit of a subscription car\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d288854",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
